# LLMguard
This project aims to develop a classifier that can classify between malicious prompts and benign prompts. A step towards secure AI and LLMs.
